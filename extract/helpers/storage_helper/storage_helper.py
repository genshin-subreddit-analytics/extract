import boto3

class StorageHelper:
    """
    Amazon S3 Interface
    """

    def __init__(self, bucket_name):
        self.bucket = boto3.client('s3')
        self.bucket_name = bucket_name

    def set_name(self, name):
        self.save_file_name = name

    def upload_to_cloud(self, file_name):
        self.bucket.upload_file(
            file_name,
            self.bucket_name, 
            file_name
        )

    def save_to_disk(self, comment_generator, batch_size, comment_cleaner):
        first_row = True
        # Open the output file in append mode
        with open(self.save_file_name, 'a') as f:
            # Iterate over the data generated by your function
            for df_chunk in comment_generator(batch_size=batch_size):
                # Write the chunk of data to the file
                comment_cleaner(df_chunk).to_csv(f, header=first_row, index=False)

                # No need for headers anymore
                first_row = False

                # Free memory by deleting the chunk
                del df_chunk

    @staticmethod
    def save_to_disk_parquet(df, file_name):
        df.to_parquet(
            path=file_name
        )
